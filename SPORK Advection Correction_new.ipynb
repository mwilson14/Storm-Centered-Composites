{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4269226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyart\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from netCDF4 import Dataset\n",
    "import nexradaws\n",
    "import os\n",
    "import metpy\n",
    "from ungridded_section_spin import quality_control_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44a314-8b65-4d7e-a00f-53fee59b95ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb411568-3ac1-49ee-b995-ba570b3133b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -1 2021    6   19    0  220]\n",
      "['KIND' '2021' '6' '19' '0' '220']\n",
      "[220 160 190 145 200 140 160 180 110 170 120 170 165 140 150 140 140 170\n",
      " 170 150 150 130 150 130 140 190 180 180 180 180 130  70 170 200 200 190\n",
      " 190 120 130 140 200 200 180 220 190 180 190 160 160 150 160 150 160 180\n",
      " 140 150 140 170 150 160 180 180 210 230 190 230 180 180 150 130 190 240\n",
      " 170 130 190 160 130 150 180 180 190 190 180 230 150 150 200 230 170 190\n",
      " 140 150 150 220 160 190 180 180 160 170 180 150 160 150]\n",
      "[180 180 180 150 180 190 140 180 130 150 160 190 150 160 160 160 170 190\n",
      " 180 180 170 140 210 160 160 220 210 210 160 180 190 170 150 180 170 180\n",
      " 190 150 170 180 160 160 160 180 160 160 210 150 180 140 120 140 170 160\n",
      " 170 150 220 170 160 190 210 210 200 200 240 210 200 220 130 210 190 210\n",
      " 230 180 210 180 150 190 180 170 160 140 140 200 200 190 210 240 190 240\n",
      " 200 160 200 190 170 190 200 190 210]\n"
     ]
    }
   ],
   "source": [
    "#Read in the sounding data from the spreadsheet\n",
    "#Bring in the sounding data\n",
    "CIsoundings=np.genfromtxt('SupercellSoundings/environment_cicases_Final.csv',skip_header=1,delimiter=',', dtype=str)\n",
    "Oldsoundings=np.genfromtxt('SupercellSoundings/environment_Oldcases.csv',delimiter=',', dtype=str)\n",
    "# ETsoundings=np.genfromtxt('PythonEnvVars1.csv', skip_header=1,delimiter=',')\n",
    "# ETsoundings[ETsoundings==-9999]=np.nan\n",
    "\n",
    "NewSoundings=np.genfromtxt('SupercellSoundingsNew/environment_cicases1022.csv', delimiter=',', dtype=str)\n",
    "\n",
    "TORsondes = pickle.load(open('TORsondes.pkl', 'rb'))\n",
    "NTORsondes = pickle.load(open('NTORsondes.pkl', 'rb'))\n",
    "ETORsondes = pickle.load(open('ETORsondes.pkl', 'rb'))\n",
    "ENTORsondes = pickle.load(open('ENTORsondes.pkl', 'rb'))\n",
    "TTorsondes = pickle.load(open('TTorsondes.pkl', 'rb'))\n",
    "TNTsondes = pickle.load(open('TNTsondes.pkl', 'rb'))\n",
    "STsondes = pickle.load(open('STsondes.pkl', 'rb'))\n",
    "SNsondes = pickle.load(open('SNsondes.pkl', 'rb'))\n",
    "\n",
    "ALL_sites1 = pickle.load(open('ALLVARSite.pkl', 'rb'))\n",
    "ALL_years1 = pickle.load(open('ALLVARYear.pkl', 'rb'))\n",
    "ALL_months1 = pickle.load(open('ALLVARMonth.pkl', 'rb'))\n",
    "ALL_days1 = pickle.load(open('ALLVARDay.pkl', 'rb'))\n",
    "ALL_hours1 = pickle.load(open('ALLVARHour.pkl', 'rb'))\n",
    "ALL_TN1 = pickle.load(open('ALLVARTor_Non.pkl', 'rb'))\n",
    "\n",
    "ALL_b_t1 = np.zeros((ALL_TN1.shape))\n",
    "ALL_b_t1[ALL_TN1=='T'] = 1.0\n",
    "\n",
    "#Fix the incorrect entry here\n",
    "ALL_sites1[44] = 'KTLX'\n",
    "ALL_years1[44] = 2019\n",
    "ALL_months1[44] = 3\n",
    "ALL_days1[44] = 23\n",
    "ALL_hours1[44] = 23\n",
    "ALL_TN1[44] = 'N'\n",
    "\n",
    "#Fix second incorrect entry here\n",
    "ALL_sites1[54] = 'KFDR'\n",
    "ALL_years1[54] = 2013\n",
    "ALL_months1[54] = 4\n",
    "ALL_days1[54] = 18\n",
    "ALL_hours1[54] = 1\n",
    "ALL_TN1[54] = 'T'\n",
    "\n",
    "#Fix third incorrect entry here\n",
    "ALL_sites1[61] = 'KENX'\n",
    "ALL_years1[61] = 2014\n",
    "ALL_months1[61] = 7\n",
    "ALL_days1[61] = 3\n",
    "ALL_hours1[61] = 21\n",
    "ALL_TN1[61] = 'N'\n",
    "\n",
    "obsd_TOR = pickle.load(open('sp6_dir_TOR.pkl', 'rb'))\n",
    "obsd_NTOR = pickle.load(open('sp6_dir_NTOR.pkl', 'rb'))\n",
    "obsd_ETOR = pickle.load(open('sp6_dir_ETOR.pkl', 'rb'))\n",
    "obsd_ENTOR = pickle.load(open('sp6_dir_ENTOR.pkl', 'rb'))\n",
    "obsd_TTor = pickle.load(open('sp6_dir_TTor.pkl', 'rb'))\n",
    "obsd_TNT = pickle.load(open('sp6_dir_TNT.pkl', 'rb'))\n",
    "obsd_ST = pickle.load(open('sp6_dir_ST.pkl', 'rb'))\n",
    "obsd_SN = pickle.load(open('sp6_dir_SN.pkl', 'rb'))\n",
    "\n",
    "obspd_TOR = pickle.load(open('sp6_sp_TOR.pkl', 'rb'))\n",
    "obspd_NTOR = pickle.load(open('sp6_sp_NTOR.pkl', 'rb'))\n",
    "obspd_ETOR = pickle.load(open('sp6_sp_ETOR.pkl', 'rb'))\n",
    "obspd_ENTOR = pickle.load(open('sp6_sp_ENTOR.pkl', 'rb'))\n",
    "obspd_TTor = pickle.load(open('sp6_sp_TTor.pkl', 'rb'))\n",
    "obspd_TNT = pickle.load(open('sp6_sp_TNT.pkl', 'rb'))\n",
    "obspd_ST = pickle.load(open('sp6_sp_ST.pkl', 'rb'))\n",
    "obspd_SN = pickle.load(open('sp6_sp_SN.pkl', 'rb'))\n",
    "\n",
    "#Create the full arrays for storm speed and direction\n",
    "#all_dir = np.concatenate([obsd_TOR, obsd_NTOR, obsd_ETOR, obsd_ENTOR, obsd_TTor, obsd_TNT, obsd_ST, obsd_SN], axis=0)\n",
    "#all_spd = np.concatenate([obspd_TOR, obspd_NTOR, obspd_ETOR, obspd_ENTOR, obspd_TTor, obspd_TNT, obspd_ST, obspd_SN], axis=0)\n",
    "\n",
    "\n",
    "NewCases = np.genfromtxt('SPORK_rerun_new.csv', delimiter=',', usecols=(0,2,3,4,7,17),skip_header=1, dtype=int)\n",
    "NewCases1 = np.genfromtxt('SPORK_rerun_new.csv', delimiter=',', usecols=(0,2,3,4,7,17), dtype=str, skip_header=1)\n",
    "\n",
    "print(NewCases[0,:])\n",
    "print(NewCases1[0,:])\n",
    "\n",
    "ALL_sites2 = NewCases1[:,0]\n",
    "ALL_years2 = NewCases[:,1]\n",
    "ALL_months2 = NewCases[:,2]\n",
    "ALL_days2 = NewCases[:,3]\n",
    "ALL_hours2 = NewCases[:,4]\n",
    "ALL_TN2 = np.copy(ALL_sites2)\n",
    "ALL_TN2[:] = 'TR'\n",
    "ffds2 = NewCases[:,5]\n",
    "print(ffds2)\n",
    "\n",
    "\n",
    "NewCases2 = np.genfromtxt('New2023CSV.csv', delimiter=',', skip_header=1, dtype=int)\n",
    "NewCases3 = np.genfromtxt('New2023CSV.csv', delimiter=',', dtype=str, skip_header=1)\n",
    "\n",
    "\n",
    "ALL_sites3 = NewCases3[:,0]\n",
    "ALL_years3 = NewCases2[:,2]\n",
    "ALL_months3 = NewCases2[:,3]\n",
    "ALL_days3 = NewCases2[:,4]\n",
    "ALL_hours3 = NewCases2[:,7]\n",
    "ALL_TN3 = np.copy(ALL_sites3)\n",
    "ALL_TN3[:] = 'TR'\n",
    "ffds3 = NewCases2[:,14]\n",
    "print(ffds3)\n",
    "#ALL_TN = pickle.load(open('ALLVARTor_Non.pkl', 'rb'))\n",
    "\n",
    "\n",
    "ALL_sites = np.concatenate([ALL_sites1, ALL_sites2, ALL_sites3], axis=0)\n",
    "ALL_years = np.concatenate([ALL_years1, ALL_years2, ALL_years3], axis=0)\n",
    "ALL_months = np.concatenate([ALL_months1, ALL_months2, ALL_months3], axis=0)\n",
    "ALL_days = np.concatenate([ALL_days1, ALL_days2, ALL_days3], axis=0)\n",
    "ALL_hours = np.concatenate([ALL_hours1, ALL_hours2, ALL_hours3], axis=0)\n",
    "ALL_TN = np.concatenate([ALL_TN1, ALL_TN2, ALL_TN3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f23a0-2872-43e6-bd3f-9c1f7ef28e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7862a7a-e36f-499f-b32c-f2ae247c78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting the radar column above a target\n",
    "given position in latitude, longitude\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pyart.core.transforms import antenna_vectors_to_cartesian\n",
    "\n",
    "\n",
    "def column_vertical_profile(\n",
    "    radar, latitude, longitude, azimuth_spread=3, spatial_spread=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the location (in latitude, longitude) of a target, return the rays\n",
    "    that correspond to radar column above the target, allowing for user\n",
    "    defined range of azimuths and range gates to be included within this\n",
    "    extraction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar : pyart.core.Radar Object\n",
    "        Py-ART Radar Object from which distance to the target, along\n",
    "        with gates above the target, will be calculated.\n",
    "    latitude : float, [degrees]\n",
    "        Latitude, in degrees North, of the target.\n",
    "    longitude : float, [degrees]\n",
    "        Longitude, in degrees East, of the target.\n",
    "    azimuth_spread : int\n",
    "        Number of azimuth angles to include within extraction list\n",
    "    spatial_range : int\n",
    "        Number of range gates to include within the extraction\n",
    "\n",
    "    Function Calls\n",
    "    --------------\n",
    "    sphere_distance\n",
    "    for_azimuth\n",
    "    get_sweep_rays\n",
    "    subset_fields\n",
    "    assemble_column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    column : xarray\n",
    "        Xarray Dataset containing the radar column above the target for\n",
    "        the various fields within the radar object.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Murphy, A. M., A. Ryzhkov, and P. Zhang, 2020: Columnar Vertical\n",
    "    Profile (CVP) Methodology for Validating Polarimetric Radar Retrievals\n",
    "    in Ice Using In Situ Aircraft Measurements. J. Atmos. Oceanic Technol.,\n",
    "    37, 1623–1642, https://doi.org/10.1175/JTECH-D-20-0011.1.\n",
    "\n",
    "    Bukovčić, P., A. Ryzhkov, and D. Zrnić, 2020: Polarimetric Relations for\n",
    "    Snow Estimation—Radar Verification. J. Appl. Meteor. Climatol.,\n",
    "    59, 991–1009, https://doi.org/10.1175/JAMC-D-19-0140.1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the spatial range to use within extraction\n",
    "    spatial_range = radar.range[\"meters_between_gates\"] * spatial_spread\n",
    "\n",
    "    # Define a dictionary structure to contain the extracted features\n",
    "    total_moment = {key: [] for key in radar.fields.keys()}\n",
    "    total_moment.update({\"height\": [], \"time_offset\": []})\n",
    "\n",
    "    # Define the start of the radar volume\n",
    "    base_time = pd.to_datetime(radar.time[\"units\"][14:]).to_numpy()\n",
    "\n",
    "    # call the sphere_distance function\n",
    "    dis = sphere_distance(\n",
    "        radar.latitude[\"data\"][0], latitude, radar.longitude[\"data\"][0], longitude\n",
    "    )\n",
    "    # calculate forward azimuth angle\n",
    "    forazi = for_azimuth(\n",
    "        radar.latitude[\"data\"][0], latitude, radar.longitude[\"data\"][0], longitude\n",
    "    )\n",
    "\n",
    "    # Iterate through radar sweeps, extract desired section\n",
    "    for sweep in radar.iter_slice():\n",
    "        moment = {key: [] for key in radar.fields.keys()}\n",
    "        zgates = []\n",
    "        gate_time = []\n",
    "\n",
    "        # call the new sweep rays\n",
    "        center, spread = get_sweep_rays(\n",
    "            radar.azimuth[\"data\"][sweep], forazi, azimuth_spread=azimuth_spread\n",
    "        )\n",
    "\n",
    "        # add the start indice of each ray\n",
    "        center = [x + sweep.start for x in center]\n",
    "        spread = [x + sweep.start for x in spread]\n",
    "        # Correct the spread indices to remove the centerline\n",
    "        spread = [x for x in spread if x not in center]\n",
    "\n",
    "        # For the ray(s) directly over the target, extract and average fields\n",
    "        for ray in center:\n",
    "            # Convert gates from antenna or cartesian coordinates\n",
    "            (rhi_x, rhi_y, rhi_z) = antenna_vectors_to_cartesian(\n",
    "                radar.range[\"data\"],\n",
    "                radar.azimuth[\"data\"][ray],\n",
    "                radar.elevation[\"data\"][ray],\n",
    "                edges=False,\n",
    "            )\n",
    "            # Calculate distance to target\n",
    "            rhidis = np.sqrt((rhi_x**2) + (rhi_y**2)) * np.sign(rhi_z)\n",
    "            # Calculate target gate\n",
    "            tar_gate = np.nonzero(np.abs(rhidis[0, :] - dis) < spatial_range)[\n",
    "                0\n",
    "            ].tolist()\n",
    "            # Subset the radar fields for the target locations\n",
    "            subset = subset_fields(radar, ray, tar_gate)\n",
    "            # Add back to the total dictionary\n",
    "            moment = {key: moment[key] + subset[key] for key in moment}\n",
    "            # Add radar elevation to height gates\n",
    "            # to define height as center of each gate above sea level\n",
    "            zgates.append(np.ma.mean(rhi_z[0, tar_gate] + radar.altitude[\"data\"][0]))\n",
    "            # Determine the time for the individual gates\n",
    "            gate_time.append(radar.time[\"data\"][ray])\n",
    "\n",
    "        # Convert to Cartesian Coordinates\n",
    "        # Determine the center of each gate for the subsetted rays.\n",
    "        for ray in spread:\n",
    "            (rhi_x, rhi_y, rhi_z) = antenna_vectors_to_cartesian(\n",
    "                radar.range[\"data\"],\n",
    "                radar.azimuth[\"data\"][ray],\n",
    "                radar.elevation[\"data\"][ray],\n",
    "                edges=False,\n",
    "            )\n",
    "            # Calculate distance to target\n",
    "            rhidis = np.sqrt((rhi_x**2) + (rhi_y**2)) * np.sign(rhi_z)\n",
    "            # Calculate target gate\n",
    "            tar_gate = np.nonzero(np.abs(rhidis[0, :] - dis) < spatial_range)[\n",
    "                0\n",
    "            ].tolist()\n",
    "            # Subset the radar fields for the target locations\n",
    "            subset = subset_fields(radar, ray, tar_gate)\n",
    "            # Add back to the sweep dictionary\n",
    "            moment = {key: moment[key] + subset[key] for key in moment}\n",
    "            # Add radar elevation to height gates\n",
    "            # to define height as center of each gate above sea level\n",
    "            zgates.append(np.ma.mean(rhi_z[0, tar_gate] + radar.altitude[\"data\"][0]))\n",
    "            # Determine the time for the individual gates\n",
    "            gate_time.append(radar.time[\"data\"][ray])\n",
    "\n",
    "        # Average all azimuth moments into a single value for the sweep\n",
    "        for key in total_moment:\n",
    "            if key == \"height\":\n",
    "                total_moment[key].append(np.ma.mean(np.ma.masked_invalid(zgates)))\n",
    "            elif key == \"time_offset\":\n",
    "                total_moment[key].append(np.round(np.ma.mean(np.array(gate_time)), 4))\n",
    "            else:\n",
    "                total_moment[key].append(\n",
    "                    np.round(np.ma.mean(np.ma.masked_invalid(moment[key])), 4)\n",
    "                )\n",
    "\n",
    "    # Add the base time for the radar\n",
    "    total_moment.update({\"base_time\": base_time})\n",
    "\n",
    "    # Convert to xarray\n",
    "    return assemble_column(radar, total_moment, forazi, dis, latitude, longitude)\n",
    "\n",
    "\n",
    "\n",
    "def sphere_distance(radar_latitude, target_latitude, radar_longitude, target_longitude):\n",
    "    \"\"\"\n",
    "    Calculated of the great circle distance between radar and target\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    Radius of the Earth = 6371 km / 6371000 meters\n",
    "    Distance is calculated for a smooth sphere\n",
    "    Radar and Target are at the same altitude (need to check)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar_latitude : float, [degrees]\n",
    "        latitude of the radar in degrees\n",
    "    target_latitude : float, [degrees]\n",
    "        latitude of the target in degrees\n",
    "    radar_longitude : float, [degrees]\n",
    "        longitude of the radar in degrees\n",
    "    target_longitude : float, [degrees]\n",
    "        longitude of the target in degress\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance : float, [meters]\n",
    "        Great-Circle Distance between radar and target in meters\n",
    "    \"\"\"\n",
    "    # check if latitude, longitudes are valid\n",
    "    check_latitude(radar_latitude)\n",
    "    check_latitude(target_latitude)\n",
    "    check_longitude(radar_longitude)\n",
    "    check_longitude(target_longitude)\n",
    "\n",
    "    # convert latitudeitude/longitudegitudes to radians\n",
    "    radar_latitude = radar_latitude * (np.pi / 180.0)\n",
    "    target_latitude = target_latitude * (np.pi / 180.0)\n",
    "    radar_longitude = radar_longitude * (np.pi / 180.0)\n",
    "    target_longitude = target_longitude * (np.pi / 180.0)\n",
    "\n",
    "    # difference in latitudeitude\n",
    "    d_latitude = target_latitude - radar_latitude\n",
    "    # difference in longitudegitude\n",
    "    d_longitude = target_longitude - radar_longitude\n",
    "\n",
    "    # Haversine formula\n",
    "    numerator = (np.sin(d_latitude / 2.0) ** 2.0) + np.cos(radar_latitude) * np.cos(\n",
    "        target_latitude\n",
    "    ) * (np.sin(d_longitude / 2.0) ** 2.0)\n",
    "    distance = 2 * 6371000 * np.arcsin(np.sqrt(numerator))\n",
    "\n",
    "    # return the output\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def for_azimuth(radar_latitude, target_latitude, radar_longitude, target_longitude):\n",
    "    \"\"\"\n",
    "    Calculation of inital bearing alongitudeg a great-circle arc\n",
    "    Known as Forward Azimuth Angle.\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    Radius of the Earth = 6371 km / 6371000 meters\n",
    "    Distance is calculatitudeed for a smooth sphere\n",
    "    Radar and Target are at the same altitude (need to check)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar_latitude : float, [degrees]\n",
    "        latitude of the radar in degrees\n",
    "    target_latitude : float, [degrees]\n",
    "        latitude of the target in degrees\n",
    "    radar_longitude : float, [degrees]\n",
    "        longitude of the radar in degrees\n",
    "    target_longitude : float, [degrees]\n",
    "        longitude of the target in degress\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    azimuth : float, [degrees]\n",
    "        azimuth angle from the radar where\n",
    "        target is located within the scan.\n",
    "        output is in degrees.\n",
    "    \"\"\"\n",
    "    # check the input latitude/longitude\n",
    "    check_latitude(radar_latitude)\n",
    "    check_latitude(target_latitude)\n",
    "    check_longitude(radar_longitude)\n",
    "    check_longitude(target_longitude)\n",
    "\n",
    "    # convert latitudeitude/longitudegitudes to radians\n",
    "    radar_latitude = radar_latitude * (np.pi / 180.0)\n",
    "    target_latitude = target_latitude * (np.pi / 180.0)\n",
    "    radar_longitude = radar_longitude * (np.pi / 180.0)\n",
    "    target_longitude = target_longitude * (np.pi / 180.0)\n",
    "\n",
    "    # Differnce in longitudegitudes\n",
    "    d_longitude = target_longitude - radar_longitude\n",
    "\n",
    "    # Determine x,y coordinates for arc tangent function\n",
    "    corr_y = np.sin(d_longitude) * np.cos(target_latitude)\n",
    "    corr_x = (np.cos(radar_latitude) * np.sin(target_latitude)) - (\n",
    "        np.sin(radar_latitude) * (np.cos(target_latitude) * np.cos(d_longitude))\n",
    "    )\n",
    "\n",
    "    # Determine forward azimuth angle\n",
    "    azimuth = np.arctan2(corr_y, corr_x) * (180.0 / np.pi)\n",
    "\n",
    "    # Return the output as a function of 0-360 degrees\n",
    "    if azimuth < 0:\n",
    "        azimuth += 360.0\n",
    "\n",
    "    return azimuth\n",
    "\n",
    "\n",
    "\n",
    "def get_field_location(radar, latitude, longitude):\n",
    "    \"\"\"\n",
    "    Given the location (in latitude, longitude) of a target, extract the\n",
    "    radar column above that point for further analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar : pyart.core.Radar Object\n",
    "        Py-ART Radar Object from which distance to the target, along\n",
    "        with gates above the target, will be calculated.\n",
    "    latitude : float, [degrees]\n",
    "        Latitude, in degrees North, of the target.\n",
    "    longitude : float, [degrees]\n",
    "        Longitude, in degrees East, of the target.\n",
    "\n",
    "    Function Calls\n",
    "    --------------\n",
    "    sphere_distance\n",
    "    for_azimuth\n",
    "    get_column_rays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    column : xarray DataSet\n",
    "        Xarray Dataset containing the radar column above the target for\n",
    "        the various fields within the radar object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure latitude, longitudes are valid\n",
    "    check_latitude(latitude)\n",
    "    check_longitude(longitude)\n",
    "\n",
    "    # initiate a dictionary to hold the gates above the radar.\n",
    "    zgate = []\n",
    "\n",
    "    # initiate a diciontary to hold the moment data.\n",
    "    moment = {key: [] for key in radar.fields.keys()}\n",
    "\n",
    "    # call the sphere_distance function\n",
    "    dis = sphere_distance(\n",
    "        radar.latitude[\"data\"][0], latitude, radar.longitude[\"data\"][0], longitude\n",
    "    )\n",
    "\n",
    "    # call the for_azimuth function\n",
    "    azim = for_azimuth(\n",
    "        radar.latitude[\"data\"][0], latitude, radar.longitude[\"data\"][0], longitude\n",
    "    )\n",
    "\n",
    "    # call the get_column_ray function\n",
    "    ray = get_column_rays(radar, azim)\n",
    "\n",
    "    # Determine the center of each gate for the subsetted rays.\n",
    "    (rhi_x, rhi_y, rhi_z) = antenna_vectors_to_cartesian(\n",
    "        radar.range[\"data\"],\n",
    "        radar.azimuth[\"data\"][ray],\n",
    "        radar.elevation[\"data\"][ray],\n",
    "        edges=False,\n",
    "    )\n",
    "    # Calculate distance from the x,y coordinates to target\n",
    "    rhidis = np.sqrt((rhi_x**2) + (rhi_y**2)) * np.sign(rhi_z)\n",
    "    for i in range(len(ray)):\n",
    "        tar_gate = np.argmin(abs(rhidis[i, 1:] - (dis)))\n",
    "        for key in moment:\n",
    "            if radar.fields[key][\"data\"][ray[i], tar_gate] is np.ma.masked:\n",
    "                moment[key].append(np.nan)\n",
    "            else:\n",
    "                moment[key].append(radar.fields[key][\"data\"][ray[i], tar_gate])\n",
    "        # Add radar elevation to height gates\n",
    "        # to define height as center of each gate above sea level\n",
    "        zgate.append(rhi_z[i, tar_gate] + radar.altitude[\"data\"][0])\n",
    "\n",
    "    # Determine the time at the center of each ray within the column\n",
    "    # Define the start of the radar volume as a numpy datetime object for xr\n",
    "    # We take advantage of the \"seconds since \" portion of the units string\n",
    "    base_time = pd.to_datetime(radar.time[\"units\"][14:]).to_numpy()\n",
    "    # Convert Py-ART radar object time (time since volume start) to time delta\n",
    "    # Add to base time to have sequential time within the xr Dataset\n",
    "    # for easier future merging/work\n",
    "    combined_time = []\n",
    "    for i in range(len(ray)):\n",
    "        delta = pd.to_timedelta(radar.time[\"data\"][ray[i]], unit=\"s\")\n",
    "        total_time = base_time + delta\n",
    "        combined_time.append(total_time.to_numpy())\n",
    "\n",
    "    # Create a blank list to hold the xarray DataArrays\n",
    "    ds_container = []\n",
    "    da_meta = [\n",
    "        \"units\",\n",
    "        \"standard_name\",\n",
    "        \"long_name\",\n",
    "        \"valid_max\",\n",
    "        \"valid_min\",\n",
    "        \"coordinates\",\n",
    "    ]\n",
    "    # Convert the moment dictionary to xarray DataArray.\n",
    "    # Apply radar object meta data to DataArray attribute\n",
    "    for key in moment:\n",
    "        if key != \"height\":\n",
    "            da = xr.DataArray(\n",
    "                moment[key], coords=dict(height=zgate), name=key, dims=[\"height\"]\n",
    "            )\n",
    "            for tag in da_meta:\n",
    "                if tag in radar.fields[key]:\n",
    "                    da.attrs[tag] = radar.fields[key][tag]\n",
    "            # Append to ds container\n",
    "            ds_container.append(da.to_dataset(name=key))\n",
    "\n",
    "    # Add additional DataArrays 'base_time' and 'time_offset'\n",
    "    # if not present within the radar object.\n",
    "    da_base = xr.DataArray(base_time, name=\"base_time\")\n",
    "    da_offset = xr.DataArray(\n",
    "        combined_time, coords=dict(height=zgate), name=\"time_offset\", dims=[\"height\"]\n",
    "    )\n",
    "    ds_container.append(da_base.to_dataset(name=\"base_time\"))\n",
    "    ds_container.append(da_offset.to_dataset(name=\"time_offset\"))\n",
    "\n",
    "    # Create a xarray DataSet from the DataArrays\n",
    "    column = xr.merge(ds_container)\n",
    "\n",
    "    # Assign Attributes for the Height and Times\n",
    "    height_des = (\n",
    "        \"Height Above Sea Level [in meters] for the Center of Each\"\n",
    "        + \" Radar Gate Above the Target Location\"\n",
    "    )\n",
    "    column.height.attrs.update(\n",
    "        long_name=\"Height of Radar Beam\",\n",
    "        units=\"m\",\n",
    "        standard_name=\"height\",\n",
    "        description=height_des,\n",
    "    )\n",
    "\n",
    "    column.base_time.attrs.update(long_name=\"UTC Reference Time\", units=\"seconds\")\n",
    "\n",
    "    time_long = \"Time in Seconds Since Volume Start\"\n",
    "    time_des = (\n",
    "        \"Time in Seconds Since Volume Start that Cooresponds\"\n",
    "        + \" to the Center of Each Height Gate\"\n",
    "        + \" Above the Target Location\"\n",
    "    )\n",
    "    column.time_offset.attrs.update(\n",
    "        long_name=time_long, units=\"seconds\", description=time_des\n",
    "    )\n",
    "\n",
    "    # Assign Global Attributes to the DataSet\n",
    "    column.attrs[\"distance_from_radar\"] = str(np.around(dis / 1000.0, 3)) + \" km\"\n",
    "    column.attrs[\"azimuth\"] = str(np.around(azim, 3)) + \" degrees\"\n",
    "    column.attrs[\"latitude_of_location\"] = str(latitude) + \" degrees\"\n",
    "    column.attrs[\"longitude_of_location\"] = str(longitude) + \" degrees\"\n",
    "    return column\n",
    "\n",
    "\n",
    "\n",
    "def get_column_rays(radar, azimuth):\n",
    "    \"\"\"\n",
    "    Given the location (in latitude,longitude) of a target, return the rays\n",
    "    that correspond to radar column above the target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar : Radar Object\n",
    "        Py-ART Radar Object from which distance to the target, along\n",
    "        with gates above the target, will be calculated.\n",
    "    azimuth : float,int\n",
    "        forward azimuth angle from radar to target in degrees.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nrays : List\n",
    "        radar ray indices that correspond to the column above a\n",
    "        target location.\n",
    "    \"\"\"\n",
    "    if isinstance(azimuth, int) or isinstance(azimuth, float) is True:\n",
    "        if (azimuth <= 0) or (azimuth >= 360):\n",
    "            raise ValueError(\"azimuth not valid (not between 0-360 degrees)\")\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"radar azimuth type not valid.\"\n",
    "            \" Please convert input to be an int or float.\"\n",
    "        )\n",
    "    # define a list to hold the valid rays\n",
    "    rays = []\n",
    "    # check to see which radar scan\n",
    "    if radar.scan_type == \"rhi\":\n",
    "        for i in range(radar.sweep_number[\"data\"].shape[0]):\n",
    "            nstart = radar.sweep_start_ray_index[\"data\"][i]\n",
    "            nstop = radar.sweep_end_ray_index[\"data\"][i]\n",
    "            counter = 0\n",
    "            for j in range(nstart, nstop):\n",
    "                if abs(radar.azimuth[\"data\"][nstart + counter] - azimuth) < 1:\n",
    "                    rays.append(nstart + counter)\n",
    "                counter += 1\n",
    "    else:\n",
    "        # taken from pyart.graph.RadarDisplay.get_azimuth_rhi_data_x_y_z\n",
    "        for sweep in radar.iter_slice():\n",
    "            sweep_azi = radar.azimuth[\"data\"][sweep]\n",
    "            nray = np.argmin(np.abs(sweep_azi - azimuth))\n",
    "            rays.append(nray + sweep.start)\n",
    "    # make sure rays were found\n",
    "    if len(rays) == 0:\n",
    "        raise ValueError(\"No rays were found between azimuth and target\")\n",
    "\n",
    "    return rays\n",
    "\n",
    "\n",
    "\n",
    "def get_sweep_rays(sweep_azi, azimuth, azimuth_spread=0):\n",
    "    \"\"\"\n",
    "    Extract the specific rays for a given azimuth from a radar sweep\n",
    "\n",
    "    Azimuth spread determines the +/- degrees azimuth to include within\n",
    "    the extraction by multipling the azimuth resolution by input value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radar_sweep : pyart.core.radar object\n",
    "        Radar Sweep from which the rays are extracted from\n",
    "    azimuth : float [degrees]\n",
    "        Forward Azimuth Angle from Radar to Target in Degreees\n",
    "    Azimuth_Spread : int\n",
    "        Number of azimuth angles to include within extraction list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    center_rays : list [integers]\n",
    "        List of integers cooresponding to ray indices within the azimuth\n",
    "        directly over the target\n",
    "    spread_rays : list [integers]\n",
    "        List of integers cooresponding to ray indices within the spread\n",
    "        of azimuths emcompassing the target\n",
    "\n",
    "    \"\"\"\n",
    "    # determine resolution of azimuth angles\n",
    "    resolution = np.round((sweep_azi[1] - sweep_azi[0]), 3)\n",
    "\n",
    "    centerline = np.nonzero(np.abs(sweep_azi - azimuth) < 0.5)[0].tolist()\n",
    "    spread = np.nonzero(np.abs(sweep_azi - azimuth) < (resolution * azimuth_spread))[\n",
    "        0\n",
    "    ].tolist()\n",
    "\n",
    "    return centerline, spread\n",
    "\n",
    "\n",
    "def subset_fields(radar, ray, target_gates):\n",
    "    \"\"\"\n",
    "    Parameter\n",
    "    ---------\n",
    "    radar : pyart.core.radar object\n",
    "        Radar Sweep from which fields are extracted from the target locations\n",
    "    target_gates : list\n",
    "        List containing indices for the gates of interest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fields : dict\n",
    "        dictionary containing averaged subset fields for target location\n",
    "\n",
    "    \"\"\"\n",
    "    # initiate a diciontary to hold the moment data.\n",
    "    moment = {key: [] for key in radar.fields.keys()}\n",
    "\n",
    "    # Iterate over input rays and average according to input method\n",
    "    # future - allow users to input weights for spatial averaging\n",
    "    for key in moment:\n",
    "        if key != \"height\":\n",
    "            if np.ma.all(radar.fields[key][\"data\"][ray, target_gates]) is np.ma.masked:\n",
    "                moment[key].append(np.nan)\n",
    "            else:\n",
    "                moment[key].append(\n",
    "                    np.ma.mean(radar.fields[key][\"data\"][ray, target_gates])\n",
    "                )\n",
    "\n",
    "    return moment\n",
    "\n",
    "\n",
    "def assemble_column(radar, total_moment, azimuth, distance, latitude, longitude):\n",
    "    \"\"\"\n",
    "    With a dictionary containing the extracted fields from a radar sweep,\n",
    "    assemble individual gates and fields into an xarray DataSet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_moment : dict\n",
    "        Dictionary containing the extracted fields from the radar object.\n",
    "        File requires at least the height of the individual gates and\n",
    "        the start time of the volumetric scan.\n",
    "    azimuth : float, [degrees]\n",
    "        azimuth angle from the radar where\n",
    "        target is located within the scan.\n",
    "        output is in degrees.\n",
    "    distance : float, [meters]\n",
    "        Great-Circle Distance between radar and target in meters\n",
    "    latitude : float, [degrees]\n",
    "        Latitude of the target in degrees\n",
    "    longitude : float, [degrees]\n",
    "        Longitude of the target in degrees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    column : xarray DataSet\n",
    "        Xarray Dataset containing the radar column above the target for\n",
    "        the various fields within the radar object.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a blank list to hold the xarray DataArrays\n",
    "    ds_container = []\n",
    "    da_meta = [\n",
    "        \"units\",\n",
    "        \"standard_name\",\n",
    "        \"long_name\",\n",
    "        \"valid_max\",\n",
    "        \"valid_min\",\n",
    "        \"coordinates\",\n",
    "    ]\n",
    "    # Skip these fields and apply meta data after creation\n",
    "    # of the Xarray DataSet\n",
    "    skip = [\"height\", \"base_time\"]\n",
    "    # Convert the moment dictionary to xarray DataArray.\n",
    "    # Apply radar object meta data to DataArray attribute\n",
    "    for key in total_moment:\n",
    "        if key not in skip:\n",
    "            # Convert Masked Array elements to NaNs for Xarray\n",
    "            total_moment[key] = [\n",
    "                np.nan if x is np.ma.masked else x for x in total_moment[key]\n",
    "            ]\n",
    "            # Convert to Xarray DataArray, set derived height as\n",
    "            # dimension/coordinates\n",
    "            da = xr.DataArray(\n",
    "                total_moment[key],\n",
    "                coords=dict(height=total_moment[\"height\"]),\n",
    "                name=key,\n",
    "                dims=[\"height\"],\n",
    "            )\n",
    "            # Add meta data for the radar fields\n",
    "            if key != \"time_offset\":\n",
    "                for tag in da_meta:\n",
    "                    if tag in radar.fields[key]:\n",
    "                        da.attrs[tag] = radar.fields[key][tag]\n",
    "            # Append to ds container\n",
    "            ds_container.append(da.to_dataset(name=key))\n",
    "\n",
    "    # Create a xarray DataSet from the DataArrays\n",
    "    column = xr.merge(ds_container)\n",
    "\n",
    "    # Add the scan times back into the merged column\n",
    "    column[\"base_time\"] = total_moment[\"base_time\"]\n",
    "    column.base_time.attrs.update(\n",
    "        long_name=(\n",
    "            \"Start time of individual radar scan volumes \"\n",
    "            + \" from which column are extracted \"\n",
    "        ),\n",
    "        units=\"UTC Time\",\n",
    "    )\n",
    "\n",
    "    # Assign Attributes for the Height and Times\n",
    "    height_des = (\n",
    "        \"Height Above Sea Level [in meters] for the Center of Each\"\n",
    "        + \" Radar Gate Above the Target Location\"\n",
    "    )\n",
    "    column.height.attrs.update(\n",
    "        long_name=\"Height of Radar Beam\",\n",
    "        units=\"m\",\n",
    "        standard_name=\"height\",\n",
    "        description=height_des,\n",
    "    )\n",
    "\n",
    "    time_long = \"Time in Seconds Since Volume Start to the Center of Each Gate\"\n",
    "    time_des = (\n",
    "        \"Time in Seconds Since Volume Start (i.e. base_time) that Cooresponds\"\n",
    "        + \" to the Center of Each Height Gate\"\n",
    "        + \" Above the Target Location\"\n",
    "    )\n",
    "    column.time_offset.attrs.update(\n",
    "        long_name=time_long, units=\"seconds\", description=time_des\n",
    "    )\n",
    "\n",
    "    # Add latitude, longitude as variable in extracted column\n",
    "    column[\"latitude\"] = latitude\n",
    "    column.latitude.attrs.update(\n",
    "        long_name=\"Latitude of Location Column is Extracted Above\", units=\"deg\"\n",
    "    )\n",
    "    column[\"longitude\"] = longitude\n",
    "    column.longitude.attrs.update(\n",
    "        long_name=\"Longitude of Location Column is Extracted Above\", units=\"deg\"\n",
    "    )\n",
    "\n",
    "    # Assign Global Attributes to the DataSet\n",
    "    column.attrs[\"distance_from_radar\"] = str(np.around(distance / 1000.0, 3)) + \" km\"\n",
    "    column.attrs[\"azimuth\"] = str(np.around(azimuth, 3)) + \" degrees\"\n",
    "    column.attrs[\"latitude_of_location\"] = str(latitude) + \" degrees\"\n",
    "    column.attrs[\"longitude_of_location\"] = str(longitude) + \" degrees\"\n",
    "\n",
    "    # Drop duplicated heights, keep latest value\n",
    "    column = column.drop_duplicates(dim=\"height\", keep=\"last\")\n",
    "\n",
    "    return column\n",
    "\n",
    "\n",
    "def check_latitude(latitude):\n",
    "    \"\"\"\n",
    "    Function to check if input latitude is valid for type and value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : int, float\n",
    "        Latitude of a location that should be between 90S and 90N\n",
    "\n",
    "    \"\"\"\n",
    "    if (\n",
    "        isinstance(latitude, int)\n",
    "        or isinstance(latitude, float)\n",
    "        or isinstance(latitude, np.floating)\n",
    "    ) is True:\n",
    "        if (latitude <= -90) or (latitude >= 90):\n",
    "            raise ValueError(\n",
    "                \"Latitude not between -90 and 90 degrees, need to \"\n",
    "                \"convert to values between -90 and 90\"\n",
    "            )\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"Latitude type not valid, need to convert input to be an int or float\"\n",
    "        )\n",
    "\n",
    "\n",
    "def check_longitude(longitude):\n",
    "    \"\"\"\n",
    "    Function to check if input latitude is valid for type and value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    longitude : int, float\n",
    "        Longitude of a location taht should be between 180W and 180E\n",
    "    \"\"\"\n",
    "    if (\n",
    "        isinstance(longitude, int)\n",
    "        or isinstance(longitude, float)\n",
    "        or isinstance(longitude, np.floating)\n",
    "    ) is True:\n",
    "        if (longitude <= -180) or (longitude >= 180):\n",
    "            raise ValueError(\n",
    "                \"Longitude not valid between -180 and 180\"\n",
    "                \" degrees, need to convert to values between\"\n",
    "                \"  -180 and 180\"\n",
    "            )\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"Longitude type not valid, need to convert input to be an int or float\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b6f848-ac9d-4d03-bc09-624e748c51ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41,)\n"
     ]
    }
   ],
   "source": [
    "z_arr = np.arange(0,10.25,0.25)\n",
    "print(z_arr.shape)\n",
    "z_am = z_arr*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7761129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grid_time_offsets(dt, end_dt, station, cenlat_arr, cenlon_arr):\n",
    "\n",
    "    n=0\n",
    "    \n",
    "    station=station\n",
    "    start_dt=dt\n",
    "    end_dt=end_dt\n",
    "    calibration=0.0\n",
    "    \n",
    "    #Here, set the initial time of the archived radar loop you want.\n",
    "    #Our specified time\n",
    "    station = station\n",
    "    #end_dt = dt + timedelta(hours=duration)\n",
    "    \n",
    "    #Set up nexrad interface\n",
    "    conn = nexradaws.NexradAwsInterface()\n",
    "    scans = conn.get_avail_scans_in_range(start_dt,end_dt,station)\n",
    "    results = conn.download(scans, 'RadarFolder')\n",
    "    \n",
    "    #Actual algorithm code starts here\n",
    "    #Create a list for the lists of arc outlines\n",
    "    zdr_out_list = []\n",
    "    heighttime_list = []\n",
    "    for i,scan in enumerate(results.iter_success(),start=1):\n",
    "    #Local file option:\n",
    "        #Loop over all files in the dataset and pull out each 0.5 degree tilt for analysis\n",
    "        try:\n",
    "            radar1 = scan.open_pyart()\n",
    "        except:\n",
    "            print('bad radar file')\n",
    "            continue\n",
    "        #Local file option\n",
    "        print('File Reading')\n",
    "    \n",
    "        #Calling ungridded_section; Pulling apart radar sweeps and creating ungridded data arrays\n",
    "        [radar,radar_v,n,range_2d,last_height,rlons_h,rlats_h,ungrid_lons,ungrid_lats] = quality_control_spin(radar1,n,calibration)\n",
    "    \n",
    "        column = column_vertical_profile(radar_v, cenlat_arr[i], cenlon_arr[i], azimuth_spread=1, spatial_spread=1)\n",
    "        print('time offsets', np.asarray(column['time_offset']), np.asarray(column['height']))\n",
    "    \n",
    "        time_interp = metpy.interpolate.interpolate_1d(z_am, np.asarray(column['height']), np.asarray(column['time_offset']))\n",
    "        time_interp[np.isnan(time_interp)]=np.nanmin(time_interp)\n",
    "        time_interp1 = time_interp-np.min(time_interp)\n",
    "        heighttime_list.append(time_interp1)\n",
    "\n",
    "        return heighttime_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdec2c96-e0e7-4d6a-b88b-bd3d6b897ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/NewSPORK_stamps/SPORK_RERUN201727KDGX.nc\n",
      "Downloaded KDGX20170207_205357_V06\n",
      "Downloaded KDGX20170207_204830_V06\n",
      "Downloaded KDGX20170207_205925_V06\n",
      "Downloaded KDGX20170207_204303_V06\n",
      "Downloaded KDGX20170207_203748_V06\n",
      "Downloaded KDGX20170207_203241_V06\n",
      "Downloaded KDGX20170207_210453_V06\n",
      "Downloaded KDGX20170207_211021_V06\n",
      "Downloaded KDGX20170207_211548_V06\n",
      "Downloaded KDGX20170207_212116_V06\n",
      "Downloaded KDGX20170207_212643_V06\n",
      "Downloaded KDGX20170207_213157_V06\n",
      "Downloaded KDGX20170207_213717_V06\n",
      "Downloaded KDGX20170207_214244_V06\n",
      "Downloaded KDGX20170207_214812_V06\n",
      "Downloaded KDGX20170207_215332_V06\n",
      "Downloaded KDGX20170207_215852_V06\n",
      "Downloaded KDGX20170207_220413_V06\n",
      "Downloaded KDGX20170207_220933_V06\n",
      "Downloaded KDGX20170207_221501_V06\n",
      "Downloaded KDGX20170207_222014_V06\n",
      "Downloaded KDGX20170207_222528_V06\n",
      "Downloaded KDGX20170207_223034_V06\n",
      "Downloaded KDGX20170207_223527_V06\n",
      "Downloaded KDGX20170207_224032_V06\n",
      "Downloaded KDGX20170207_224552_V06\n",
      "Downloaded KDGX20170207_225111_V06\n",
      "Downloaded KDGX20170207_225632_V06\n",
      "Downloaded KDGX20170207_230159_V06\n",
      "Downloaded KDGX20170207_230720_V06\n",
      "Downloaded KDGX20170207_231225_V06\n",
      "31 out of 31 files downloaded...0 errors\n",
      "File Reading\n",
      "Pre-grid Organization Section\n",
      "0.52869797\n",
      "bad split cut\n",
      "0.48339844\n",
      "0.87872314\n",
      "bad split cut\n",
      "0.87890625\n",
      "1.332386\n",
      "bad split cut\n",
      "1.3390312\n",
      "1.8095322\n",
      "2.415886\n",
      "3.1255262\n",
      "0.48495865\n",
      "bad split cut\n",
      "0.48339844\n",
      "4.0490494\n",
      "5.091339\n",
      "6.467964\n",
      "8.010361\n",
      "10.064713\n",
      "12.527222\n",
      "15.649498\n",
      "19.509033\n",
      "time offsets [ 66.372  101.322  117.342  131.3735 144.911  176.99   193.8755 207.2835\n",
      " 220.04   232.794  257.7525 270.1705 282.5975 295.0235] [  847.5   1166.5   1455.5   1860.5   2337.5    586.5   2948.5   3616.5\n",
      "  4567.    5676.25  6996.5   8672.   10852.5  13712.  ]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'metpy' has no attribute 'interpolate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     ts2 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mfromtimestamp(times[j])\n\u001b[0;32m     33\u001b[0m     ts1_arr\u001b[38;5;241m.\u001b[39mappend(ts2)\n\u001b[1;32m---> 35\u001b[0m heighttimes \u001b[38;5;241m=\u001b[39m \u001b[43mget_grid_time_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts1_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts1_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mALL_sites\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat_st\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon_st\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#         ncfile = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i])-1)+str(ALL_sites[i])+'.nc')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#         print(j)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#         j=j+1\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 41\u001b[0m, in \u001b[0;36mget_grid_time_offsets\u001b[1;34m(dt, end_dt, station, cenlat_arr, cenlon_arr)\u001b[0m\n\u001b[0;32m     38\u001b[0m column \u001b[38;5;241m=\u001b[39m column_vertical_profile(radar_v, cenlat_arr[i], cenlon_arr[i], azimuth_spread\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, spatial_spread\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime offsets\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(column[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_offset\u001b[39m\u001b[38;5;124m'\u001b[39m]), np\u001b[38;5;241m.\u001b[39masarray(column[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m---> 41\u001b[0m time_interp \u001b[38;5;241m=\u001b[39m \u001b[43mmetpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[38;5;241m.\u001b[39minterpolate_1d(z_am, np\u001b[38;5;241m.\u001b[39masarray(column[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m]), np\u001b[38;5;241m.\u001b[39masarray(column[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_offset\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     42\u001b[0m time_interp[np\u001b[38;5;241m.\u001b[39misnan(time_interp)]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnanmin(time_interp)\n\u001b[0;32m     43\u001b[0m time_interp1 \u001b[38;5;241m=\u001b[39m time_interp\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(time_interp)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'metpy' has no attribute 'interpolate'"
     ]
    }
   ],
   "source": [
    "#Generate the file names for the postage stamps\n",
    "j=1\n",
    "#for i in range(len(ALL_years)):\n",
    "#for i in np.arange(201,207,1):\n",
    "for i in [0]:\n",
    "    #try:\n",
    "    print('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "    ncfile = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "\n",
    "    ref = ncfile.variables['REFL'][:]\n",
    "    kdp = ncfile.variables['KDP'][:]\n",
    "    zdr = ncfile.variables['ZDR'][:]\n",
    "    cc = ncfile.variables['CC'][:]\n",
    "    zdr[ref < 20] = np.nan\n",
    "    zdr[cc < 0.9] = np.nan\n",
    "    kdp[ref < 20] = np.nan\n",
    "    kdp[cc < 0.9] = np.nan\n",
    "    nzdr = ncfile.variables['NZDR'][:]\n",
    "    nzdr[ref[:,4,:,:] < 20] = 0.0\n",
    "    nzdr[cc[:,4,:,:] < 0.95] = 0.0\n",
    "    zdrd = ncfile.variables['ZDRD'][:]\n",
    "    rot = ncfile.variables['ROT'][:]\n",
    "    times = ncfile.variables['Times'][:]\n",
    "    azim = ncfile.variables['azim'][:]\n",
    "    vel = ncfile.variables['VEL'][:]\n",
    "    lon_st = ncfile.variables['Lons'][:]\n",
    "    lat_st = ncfile.variables['Lats'][:]\n",
    "\n",
    "    ts1_arr = []\n",
    "    for j in range(len(times)):\n",
    "\n",
    "        ts2 = datetime.fromtimestamp(times[j])\n",
    "        ts1_arr.append(ts2)\n",
    "\n",
    "    heighttimes = get_grid_time_offsets(ts1_arr[0], ts1_arr[-1], str(ALL_sites[i]), lat_st[:,70,70], lon_st[:,70,70])\n",
    "    # except:\n",
    "    #     try:\n",
    "    #         ncfile = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i])-1)+str(ALL_sites[i])+'.nc')\n",
    "    #     except:\n",
    "    #         print('SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc is missing!')\n",
    "    #         print(j)\n",
    "    #         j=j+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1720344-2fc5-4def-85ea-6b3140df3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-07 20:32:41\n"
     ]
    }
   ],
   "source": [
    "print(ts1_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bd67bf9-63fb-4d9f-9559-ed5942a956d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.30880306 32.30881648 32.30882967 ... 32.30855795 32.30854073\n",
      "  32.30852328]\n",
      " [32.31330564 32.31331906 32.31333225 ... 32.3130605  32.31304328\n",
      "  32.31302583]\n",
      " [32.31780821 32.31782163 32.31783483 ... 32.31756305 32.31754583\n",
      "  32.31752838]\n",
      " ...\n",
      " [32.92565597 32.92566955 32.92568292 ... 32.92540776 32.92539032\n",
      "  32.92537266]\n",
      " [32.93015854 32.93017213 32.93018549 ... 32.92991031 32.92989287\n",
      "  32.92987521]\n",
      " [32.93466112 32.93467471 32.93468807 ... 32.93441287 32.93439543\n",
      "  32.93437776]]\n"
     ]
    }
   ],
   "source": [
    "print(lat_st[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dc82d97-063d-457d-81d3-abe323ff8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-89.93365596046326\n"
     ]
    }
   ],
   "source": [
    "print(lon_st[0,70,70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91034b77-d8d1-475d-bacd-e639248a49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         nan          nan          nan          nan 106.84530018\n",
      " 150.24925    170.10574268 201.78834477 231.51454363 237.64419162\n",
      " 243.96121671 250.63875096 256.2505172  260.86660462 265.48269205\n",
      " 272.3758565  280.12841366 287.88097082 297.73371706 310.71062518\n",
      " 323.6875333  336.66444142 346.5766375  348.91218437 351.24773125\n",
      " 353.58327812 355.918825   358.18466537 360.1052926  362.02591982\n",
      " 363.94654704 365.86717426 367.78780149 369.70842871 371.15239494\n",
      " 372.58762401 374.02285308 375.45808215 376.89331122 378.32854028\n",
      " 379.76376935]\n",
      "[  0.           0.           0.           0.           0.\n",
      "  43.40394982  63.2604425   94.94304459 124.66924345 130.79889144\n",
      " 137.11591653 143.79345078 149.40521702 154.02130444 158.63739186\n",
      " 165.53055632 173.28311348 181.03567064 190.88841688 203.865325\n",
      " 216.84223312 229.81914124 239.73133732 242.06688419 244.40243107\n",
      " 246.73797794 249.07352482 251.33936519 253.25999241 255.18061964\n",
      " 257.10124686 259.02187408 260.9425013  262.86312853 264.30709476\n",
      " 265.74232383 267.1775529  268.61278197 270.04801103 271.4832401\n",
      " 272.91846917]\n",
      "216.84223311844192\n"
     ]
    }
   ],
   "source": [
    "print(time_interp)\n",
    "time_interp[np.isnan(time_interp)]=np.nanmin(time_interp)\n",
    "print(time_interp-np.nanmin(time_interp))\n",
    "\n",
    "print(time_interp[20]-np.nanmin(time_interp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7220a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = column_vertical_profile(radar, 36.27, -87.06, azimuth_spread=1, spatial_spread=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1534a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  665.5  1351.5  1729.5  2243.5  2791.5  3476.5   734.   1043.   4275.\n",
      "  5351.   6639.   8238.5 10196.  12861.5 16187.5]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(column['height']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e58bd398-8989-4c32-bfbb-6bb73cf1c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-86.5625\n"
     ]
    }
   ],
   "source": [
    "print(radar.longitude['data'][:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c0c83-1c68-485c-953f-95b494c885ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6660c-e6e5-4606-9a35-90b25872b367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
