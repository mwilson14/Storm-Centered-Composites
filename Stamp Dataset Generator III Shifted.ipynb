{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d840fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\xarray\\core\\pycompat.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  duck_array_version = LooseVersion(duck_array_module.__version__)\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\xarray\\core\\pycompat.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  duck_array_version = LooseVersion(\"0.0.0\")\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\xarray\\core\\pycompat.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  duck_array_version = LooseVersion(\"0.0.0\")\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\xarray\\core\\npcompat.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) >= \"1.20.0\":\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\xarray\\core\\pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) < \"0.25.0\":\n",
      "C:\\Users\\matts\\anaconda3\\envs\\spork2022\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy.ma as ma\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import pyart\n",
    "import matplotlib.lines as mlines\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from metpy.interpolate.grid import natural_neighbor_to_grid\n",
    "from metpy.interpolate import interpolate_to_points\n",
    "from metpy.interpolate import interpolate_to_grid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2017 thunderhoser\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import numpy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def _run_pmm_one_variable(\n",
    "        input_matrix, max_percentile_level=100):\n",
    "    \"\"\"Applies PMM to one variable.\n",
    "    E = number of examples (realizations over which to average)\n",
    "    :param input_matrix: numpy array.  The first axis must have length E.  Other\n",
    "        axes are assumed to be spatial dimensions.  Thus, input_matrix[i, ...]\n",
    "        is the spatial field for the [i]th example.\n",
    "    :param max_percentile_level: Maximum percentile.  No output value will\n",
    "        exceed the [q]th percentile of `input_matrix`, where q =\n",
    "        `max_percentile_level`.  Similarly, no output value will be less than\n",
    "        the [100 - q]th percentile of `input_matrix`.\n",
    "    :return: mean_field_matrix: numpy array of probability-matched means.  Will\n",
    "        have the same dimensions as `input_matrix`, except without the first\n",
    "        axis.  For example, if `input_matrix` is E x 32 x 32 x 12, this will be\n",
    "        32 x 32 x 12.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pool values over all dimensions and remove extremes.\n",
    "    pooled_values = numpy.ravel(input_matrix)\n",
    "    pooled_values = numpy.sort(pooled_values)\n",
    "\n",
    "    max_pooled_value = numpy.percentile(pooled_values, max_percentile_level)\n",
    "    pooled_values = pooled_values[pooled_values <= max_pooled_value]\n",
    "\n",
    "    min_pooled_value = numpy.percentile(\n",
    "        pooled_values, 100 - max_percentile_level)\n",
    "    pooled_values = pooled_values[pooled_values >= min_pooled_value]\n",
    "\n",
    "    # Find ensemble mean at each grid point.\n",
    "    mean_field_matrix = numpy.mean(input_matrix, axis=0)\n",
    "    mean_field_flattened = numpy.ravel(mean_field_matrix)\n",
    "\n",
    "    # At each grid point, replace ensemble mean with the same percentile from\n",
    "    # pooled array.\n",
    "    pooled_value_percentiles = numpy.linspace(\n",
    "        0, 100, num=len(pooled_values), dtype=float)\n",
    "    mean_value_percentiles = numpy.linspace(\n",
    "        0, 100, num=len(mean_field_flattened), dtype=float)\n",
    "\n",
    "    sort_indices = numpy.argsort(mean_field_flattened)\n",
    "    unsort_indices = numpy.argsort(sort_indices)\n",
    "\n",
    "    interp_object = interp1d(\n",
    "        pooled_value_percentiles, pooled_values, kind='linear',\n",
    "        bounds_error=True, assume_sorted=True)\n",
    "\n",
    "    mean_field_flattened = interp_object(mean_value_percentiles)\n",
    "    mean_field_flattened = mean_field_flattened[unsort_indices]\n",
    "    mean_field_matrix = numpy.reshape(\n",
    "        mean_field_flattened, mean_field_matrix.shape)\n",
    "\n",
    "    return mean_field_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641d1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotation code from https://stackoverflow.com/questions/29708840/rotate-meshgrid-with-numpy\n",
    "def DoRotation(xspan, yspan, RotRad=0):\n",
    "    \"\"\"Generate a meshgrid and rotate it by RotRad radians.\"\"\"\n",
    "\n",
    "    # Clockwise, 2D rotation matrix\n",
    "    RotMatrix = np.array([[np.cos(RotRad),  np.sin(RotRad)],\n",
    "                          [-np.sin(RotRad), np.cos(RotRad)]])\n",
    "\n",
    "    x, y = np.meshgrid(xspan, yspan)\n",
    "    return np.einsum('ji, mni -> jmn', RotMatrix, np.dstack([x, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bd4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -1   -1 2023    2   21    2   21   21    0    1   40   45    0  300\n",
      "  180 3500    0]\n",
      "['KDIX' 'KDIX' '2023' '2' '21' '2' '21' '21' '0' '1.5' '40' '45'\n",
      " '-0.141302954' '300' '180' '3500' '0']\n",
      "[180 180 180 150 180 190 140 180 130 150 160 190 150 160 160 160 170 190\n",
      " 180 180 170 140 210 160 160 220 210 210 160 180 190 170 150 180 170 180\n",
      " 190 150 170 180 160 160 160 180 160 160 210 150 180 140 120 140 170 160\n",
      " 170 150 220 170 160 190 210 210 200 200 240 210 200 220 130 210 190 210\n",
      " 230 180 210 180 150 190 180 170 160 140 140 200 200 190 210 240 190 240\n",
      " 200 160 200 190 170 190 200 190 210]\n"
     ]
    }
   ],
   "source": [
    "NewCases = np.genfromtxt('New2023CSV.csv', delimiter=',', skip_header=1, dtype=int)\n",
    "NewCases1 = np.genfromtxt('New2023CSV.csv', delimiter=',', dtype=str, skip_header=1)\n",
    "\n",
    "print(NewCases[0,:])\n",
    "print(NewCases1[0,:])\n",
    "\n",
    "ALL_sites = NewCases1[:,0]\n",
    "ALL_years = NewCases[:,2]\n",
    "ALL_months = NewCases[:,3]\n",
    "ALL_days = NewCases[:,4]\n",
    "ALL_hours = NewCases[:,7]\n",
    "ffds = NewCases[:,14]\n",
    "print(ffds)\n",
    "#ALL_TN = pickle.load(open('ALLVARTor_Non.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fc7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_stamp(ffd, ncfile, xoff, yoff):\n",
    "    ref = ncfile.variables['REFL'][:]\n",
    "    kdp = ncfile.variables['KDP'][:]\n",
    "    zdr = ncfile.variables['ZDR'][:]\n",
    "    cc = ncfile.variables['CC'][:]\n",
    "    zdr[ref < 20] = np.nan\n",
    "    zdr[cc < 0.9] = np.nan\n",
    "    kdp[ref < 20] = np.nan\n",
    "    kdp[cc < 0.9] = np.nan\n",
    "    nzdr = ncfile.variables['NZDR'][:]\n",
    "    nzdr[ref[:,4,:,:] < 20] = 0.0\n",
    "    nzdr[cc[:,4,:,:] < 0.95] = 0.0\n",
    "    zdrd = ncfile.variables['ZDRD'][:]\n",
    "    rot = ncfile.variables['ROT'][:]\n",
    "    times = ncfile.variables['Times'][:]\n",
    "    azim = ncfile.variables['azim'][:]\n",
    "    vel = ncfile.variables['VEL'][:]\n",
    "    \n",
    "    try:\n",
    "        ncfile7 = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i]))+str(ALL_sites[i])+'.nc')\n",
    "        times = ncfile7.variables['Times'][:]\n",
    "\n",
    "    except:\n",
    "        ncfile7 = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i])-1)+str(ALL_sites[i])+'.nc')\n",
    "        times = ncfile7.variables['Times'][:]\n",
    "\n",
    "\n",
    "    st_1 = datetime(int(ALL_years[i]), int(ALL_months[i]), int(ALL_days[i]), int(ALL_hours[i]), 0)-timedelta(minutes=30)\n",
    "    print(st_1)\n",
    "    time_diffs = []\n",
    "    for j in range(len(times)):\n",
    "\n",
    "        time_diff = (datetime.fromtimestamp(times[j]) - st_1).total_seconds()\n",
    "        time_diffs.append(time_diff)\n",
    "    time_array = np.asarray(time_diffs)  \n",
    "    time_array[time_array<0] = 9999\n",
    "    print(time_array)\n",
    "    print(time_diffs)\n",
    "    print(np.where(time_array < 3600))\n",
    "    print(ref[time_array < 3600, :, :, :].shape[0], 'times')\n",
    "\n",
    "    ref_mn = ref[time_array < 3600, :, :, :]\n",
    "    zdr_mn = zdr[time_array < 3600, :, :, :]\n",
    "    kdp_mn = kdp[time_array < 3600, :, :, :]\n",
    "    cc_mn = cc[time_array < 3600, :, :, :]\n",
    "    rot_mn = rot[time_array < 3600, :, :, :]\n",
    "    zdrd_mn = zdrd[time_array < 3600, :, :]\n",
    "    nzdr_mn = nzdr[time_array < 3600, :, :]\n",
    "\n",
    "    ref_st = np.nanmean(ref_mn, axis=0)\n",
    "    zdr_st = np.nanmean(zdr_mn, axis=0)\n",
    "    kdp_st = np.nanmean(kdp_mn, axis=0)\n",
    "    cc_st = np.nanmean(cc_mn, axis=0)\n",
    "    rot_st = np.nanmean(rot_mn, axis=0)\n",
    "    zdrd_st = np.nanmean(zdrd_mn, axis=0)\n",
    "    nzdr_st = np.nanmean(nzdr_mn, axis=0)\n",
    "\n",
    "    nzdr_mn[np.isnan(nzdr_mn)] = 0.0\n",
    "    zdrd_mn[np.isnan(zdrd_mn)] = 0.0\n",
    "    zdr_mn[np.isnan(zdr_mn)] = 0.0\n",
    "    kdp_mn[np.isnan(kdp_mn)] = 0.0\n",
    "    cc_mn[np.isnan(cc_mn)] = 0.0\n",
    "    rot_mn[ref_mn < 20.0] = 0.0\n",
    "\n",
    "    ref_pm = _run_pmm_one_variable(ref_mn)\n",
    "    rot_pm = _run_pmm_one_variable(rot_mn)\n",
    "    zdrd_pm =  _run_pmm_one_variable(zdrd_mn)\n",
    "    nzdr_pm =  _run_pmm_one_variable(nzdr_mn)\n",
    "    zdr_pm =  _run_pmm_one_variable(zdr_mn)\n",
    "    kdp_pm =  _run_pmm_one_variable(kdp_mn)\n",
    "    cc_pm =  _run_pmm_one_variable(cc_mn)\n",
    "    for q in range(zdr_mn.shape[0]):\n",
    "        print(np.max(rot_mn[q,12,:,:]))\n",
    "    print(ref_mn.shape)\n",
    "    xr = np.arange(-70,70,1)\n",
    "    xra1, yra1 = DoRotation(xr, xr, RotRad=np.deg2rad((ffd)-180)*-1)\n",
    "    xri, yri, Z_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(ref_pm[4,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, rot1_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(rot_pm[4,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, rot3_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(rot_pm[12,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, rot5_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(rot_pm[20,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, rot7_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(rot_pm[28,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, zdrd_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(zdrd_pm[:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, zdr_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(zdr_pm[4,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, nzdr_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(nzdr_pm[:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, kdp_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(kdp_pm[4,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    xri, yri, cc_interpi = interpolate_to_grid(np.ndarray.flatten(xra1), np.ndarray.flatten(yra1),\n",
    "                                             np.ndarray.flatten(cc_pm[4,:,:]), hres=1,\n",
    "                                             boundary_coords={'west':-50+xoff,'south':-50+yoff,'east':49+xoff,'north':49+yoff})\n",
    "    return xri, yri, Z_interpi, rot1_interpi, rot3_interpi, rot5_interpi, rot7_interpi, zdrd_interpi, nzdr_interpi, zdr_interpi, kdp_interpi, cc_interpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03be21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the list of meso locations\n",
    "meso_locs = np.genfromtxt('MesocycloneLocationsPass3.csv', delimiter=',', skip_header=1)\n",
    "meso_x = meso_locs[310:,1]\n",
    "meso_y = meso_locs[310:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3fcaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(meso_x))\n",
    "print(len(ALL_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3a68e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/SPORK_AC/SPORK_AC2023622KFTG.nc\n",
      "2023-06-22 20:30:00\n",
      "[9999. 9999. 9999. 9999.  257.  530.  803. 1072. 1340. 1603. 1866. 2147.\n",
      " 2503. 2785. 3067. 3346. 3669. 3993. 4288.]\n",
      "[-812.0, -554.0, -281.0, -7.0, 257.0, 530.0, 803.0, 1072.0, 1340.0, 1603.0, 1866.0, 2147.0, 2503.0, 2785.0, 3067.0, 3346.0, 3669.0, 3993.0, 4288.0]\n",
      "(array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15], dtype=int64),)\n",
      "12 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matts\\AppData\\Local\\Temp\\ipykernel_39352\\165104713.py:51: RuntimeWarning: Mean of empty slice\n",
      "  zdr_st = np.nanmean(zdr_mn, axis=0)\n",
      "C:\\Users\\matts\\AppData\\Local\\Temp\\ipykernel_39352\\165104713.py:52: RuntimeWarning: Mean of empty slice\n",
      "  kdp_st = np.nanmean(kdp_mn, axis=0)\n",
      "C:\\Users\\matts\\AppData\\Local\\Temp\\ipykernel_39352\\165104713.py:55: RuntimeWarning: Mean of empty slice\n",
      "  zdrd_st = np.nanmean(zdrd_mn, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006043414783441606\n",
      "0.005345215473165853\n",
      "0.005558861171376096\n",
      "0.005341558642613592\n",
      "0.004471534951205777\n",
      "0.00536084177234716\n",
      "0.006644600388764274\n",
      "0.008161337739235278\n",
      "0.008068613207851702\n",
      "0.01050676902007473\n",
      "0.010271419444860132\n",
      "0.01526548996130343\n",
      "(12, 41, 140, 140)\n"
     ]
    }
   ],
   "source": [
    "#Generate the file names for the postage stamps\n",
    "j=1\n",
    "#for i in range(len(ALL_years)):\n",
    "#for i in np.arange(1,99,1):\n",
    "for i in [75]:\n",
    "    try:\n",
    "        print('D:/SPORK_AC/SPORK_AC'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "        ncfile = Dataset('D:/SPORK_AC/SPORK_AC'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "    except:\n",
    "        try:\n",
    "            ncfile = Dataset('D:/SPORK_AC/SPORK_AC'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i])-1)+str(ALL_sites[i])+'.nc')\n",
    "        except:\n",
    "            print('SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc is missing!')\n",
    "            print(j)\n",
    "            j=j+1\n",
    "    try:\n",
    "        rotated_data = rotate_stamp(ffds[i], ncfile, meso_x[i], meso_y[i]\n",
    "        with open('SPORK_SHIFT_AC3'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.pkl', 'wb') as f:\n",
    "            pickle.dump(rotated_data, f)\n",
    "    except:\n",
    "        print('SPORK_40_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc failed!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ef23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(24,24))\n",
    "xr2 = np.arange(0, len(rotated_data[0]), 1)\n",
    "reflevs = np.arange(20, 70, 5)\n",
    "zdrdlev = np.arange(1, 10, 1)\n",
    "rotlev = np.arange(0.002,0.01, 0.002)\n",
    "# plt.contourf(xr, xr, ref_st[4, :, :], reflevs, cmap=pyart.graph.cm_colorblind.HomeyerRainbow)\n",
    "# plt.contour(xr, xr, zdrd_st[:, :]/4, zdrdlev, cmap=plt.cm.viridis, linewidths=5)\n",
    "plt.contourf(rotated_data[0], rotated_data[1], rotated_data[2], reflevs, cmap=pyart.graph.cm_colorblind.HomeyerRainbow)\n",
    "plt.contourf(rotated_data[0], rotated_data[1], rotated_data[7]/4, zdrdlev, cmap=plt.cm.viridis)\n",
    "plt.contour(rotated_data[0], rotated_data[1], rotated_data[3], rotlev, colors='r', linewidths=4)\n",
    "plt.contour(rotated_data[0], rotated_data[1], rotated_data[4], rotlev, colors='purple', linewidths=4)\n",
    "plt.contour(rotated_data[0], rotated_data[1], rotated_data[5], rotlev, colors='blue', linewidths=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696428f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Geod\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Create geod object for later distance and area calculations\n",
    "g = Geod(ellps='sphere')\n",
    "def radar_motion(start_time, end_time, sloni, slati, slone, slate):\n",
    "\n",
    "    stormdte = (np.datetime64(end_time) - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "    stormdtce = datetime.utcfromtimestamp(stormdte)\n",
    "    stormdti = (np.datetime64(start_time) - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "    stormdtci = datetime.utcfromtimestamp(stormdti)\n",
    "\n",
    "    distance_track = g.inv(sloni, slati,\n",
    "                           slone, slate)\n",
    "    dist_track = distance_track[2]                                        \n",
    "    if distance_track[1] < 0:\n",
    "        back = distance_track[1] + 360\n",
    "    else:\n",
    "        back = distance_track[1]\n",
    "    storm_dur = stormdtce-stormdtci\n",
    "    storm_sec = storm_dur.seconds\n",
    "    speed = dist_track/storm_sec\n",
    "    direc = back\n",
    "    return speed, direc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f53927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get observed motions for all of these cases\n",
    "#Generate the file names for the postage stamps\n",
    "j=1\n",
    "new_speeds = []\n",
    "new_directions = []\n",
    "\n",
    "for i in range(len(ALL_years)):\n",
    "#for i in np.arange(201,207,1):\n",
    "#for i in [104]:\n",
    "    try:\n",
    "        print('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "        ncfile = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc')\n",
    "    except:\n",
    "        try:\n",
    "            ncfile = Dataset('D:/NewSPORK_stamps/SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(int(ALL_days[i])-1)+str(ALL_sites[i])+'.nc')\n",
    "        except:\n",
    "            print('SPORK_RERUN'+str(ALL_years[i])+str(ALL_months[i])+str(ALL_days[i])+str(ALL_sites[i])+'.nc is missing!')\n",
    "            print(j)\n",
    "            j=j+1\n",
    "    case_loni = ncfile.variables['Lons'][:,70,70] \n",
    "    case_lati = ncfile.variables['Lats'][:,70,70] \n",
    "    case_timesi = ncfile.variables['Times'][:] \n",
    "    #print(case_loni, case_lati, case_timesi)\n",
    "    \n",
    "    st_1 = datetime(int(ALL_years[i]), int(ALL_months[i]), int(ALL_days[i]), int(ALL_hours[i]), 0)-timedelta(minutes=30)\n",
    "    #print(st_1)\n",
    "    time_diffs = []\n",
    "    for j in range(len(case_timesi)):\n",
    "\n",
    "        time_diff = (datetime.fromtimestamp(case_timesi[j]) - st_1).total_seconds()\n",
    "        time_diffs.append(time_diff)\n",
    "    time_array = np.asarray(time_diffs)  \n",
    "    time_array[time_array<0] = 9999\n",
    "    #print(time_array)\n",
    "    #print(time_diffs)\n",
    "    #print(np.where(time_array < 3600))\n",
    "    \n",
    "    case_lon = case_loni[time_array < 3600]\n",
    "    case_lat = case_lati[time_array < 3600]\n",
    "    case_times = case_timesi[time_array < 3600]\n",
    "    \n",
    "    Xlon = np.array(case_lon)\n",
    "    Ylat = np.array(case_lat)\n",
    "    X_obs1 = Xlon[0]\n",
    "    X_obs2 = Xlon[-1]\n",
    "    X_mean = np.mean(Xlon)\n",
    "    Y_mean = np.mean(Ylat)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in range (len(Xlon)):\n",
    "        num += (Xlon[i] - X_mean)*(Ylat[i] - Y_mean)\n",
    "        den += (Xlon[i] - X_mean)**2\n",
    "    m = num / den\n",
    "    c = Y_mean - m*X_mean\n",
    "    Y_pred2 = m*X_obs2 + c\n",
    "    Y_pred1 = m*X_obs1 + c\n",
    "    #print(\"X_obs1, Y_pred1 =\", X_obs1, Y_pred1, \"; X_obs2, Y_pred2 =\", X_obs2, Y_pred2)\n",
    "    \n",
    "    st_speed, st_direction = radar_motion(datetime.fromtimestamp(case_times[0]), datetime.fromtimestamp(case_times[-1]), X_obs1, Y_pred1, X_obs2, Y_pred2)\n",
    "    \n",
    "    print(st_speed, st_direction)\n",
    "    \n",
    "    new_speeds.append(st_speed)\n",
    "    new_directions.append(st_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncfile.variables['Lons'][:,70,70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xlon)\n",
    "print(Xlon[0])\n",
    "print(Xlon[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f86413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(case_times[0])\n",
    "timestamp = case_times[0]\n",
    "dt_object = datetime.fromtimestamp(timestamp)\n",
    "print(dt_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0698d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_speeds)\n",
    "print(new_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770380c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "efilename = 'NewObsMotions.csv'\n",
    "with open(efilename, mode='w') as cal_file:\n",
    "    calWriter = csv.writer(cal_file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "\n",
    "for i in range(len(ALL_years)):\n",
    "\n",
    "    with open(efilename, mode='a') as cal_file:\n",
    "            calWriter = csv.writer(cal_file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "            calWriter.writerow([ALL_years[i], ALL_months[i], ALL_days[i], ALL_hours[i], new_speeds[i], new_directions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d29c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
